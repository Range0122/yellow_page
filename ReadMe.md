#YellowPage Spider
一个用来爬取[黄页88](http://www.huangye88.com/)的所有企业名称的爬虫爬虫。
##局限性
尽管把企业筛选范围精确到最大程度，由于网页在企业列表的1000页以后都会自动跳转到当前列表的第一页，所以仍然会有很小一部分企业遗漏掉（即超过20000的部分）
初步先完成了数据的爬取，计划稍迟一点将`yp_spider.py`中的几个`parse（）`函数进行整合简化封装。
##17.3.12更新
由于`parse_fist/second/third/forth()`的代码相似度非常高，就想着做成一个循环，或者`pase_page()`，不断调用`call_back`来循环执行。这样就需要一个*xpath*的列表，然后层层往下爬取得时候自动按照层数从列表中取出*xpath* ，并且还需要一个*n_flag* 来指示从列表中取出哪一个*xpath*。所以就采用了*meta*的方法来传递*xpath_list*和*n_flag*，同时因为需要判断当前范围内的企业数量是否达到了全部可显示的范围内（20000），还需要进行*if/elif*判断，以及使用*try/except*来避免因为报错（前两级的网页是没有*n_flag*这个值的，也就是说抓取不到这个数值）而停止。这样一来代码就显得十分臃肿，很多判断语句挤在一个`parse_page()`函数里面，而且还意外发现行数与之前的代码一样。另外就是改过代码之后死活`yield`不到`requests`里面全，能爬网站，但是完全没有最后一步的获取公司名称以及写入，最开始以为是`robox.txt`或者*user_agent*的问题被反爬虫了，在网上找了个中间件随机选取*user_agent*，然而并无变化。在`Initial commit`的代码中竟然还出现了一些错误，可能是顺序的影响。
所以代码就不打算上传了，应该也不会再作更新。毕竟只是为了获取企业的名字，接下来获取详细的企业数据信息才是重点。
##17.3.13更新
意外发现重大的bug，爬取网站只停留在了第一页，并没有对于后续（下一页）的跟进，进行了相应代码的补充。
在追求效率最大的同时，由于爬取速度过快，导致被目标网站封了IP，所以重写了`middleware`:

* `RandomUserAgentMiddleware`:从 *user_agents_list* 中随机选取ua，模拟浏览器操作，防止被ban
* `ProxyMiddleware`:从 *proxy_list* 中随机选取 *proxy*，修改IP地址，防止被ban

另外就是修改了`settings.py`中的一些设置，比如*DOWNLOAD_DELAY* 和 *COOKIES_ENABLED* 进一步防止被ban